{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24274ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch_geometric.data import Batch\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torchdiffeq import odeint\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "from dtaidistance import dtw\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e74e61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_model(model_path: str, model_class, **model_kwargs):\n",
    "    \"\"\"Load trained GraphODE model\"\"\"\n",
    "    model = model_class(**model_kwargs)\n",
    "    model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def predict_trajectories(model, test_loader, device='cpu', max_batches=None):\n",
    "    \"\"\"Generate predictions for test dataset\"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    all_batch_info = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(test_loader):\n",
    "            if max_batches and batch_idx >= max_batches:\n",
    "                break\n",
    "                \n",
    "            batch.graphs = batch.graphs.to(device)\n",
    "            batch.next_positions = batch.next_positions.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            time_span = torch.tensor([0., 1.], device=device)\n",
    "            result = model(batch.graphs, time_span)\n",
    "            pred_next_positions = result['trajectories'][1]  # t=1 predictions\n",
    "            \n",
    "            # Store results\n",
    "            all_predictions.append(pred_next_positions.cpu().numpy())\n",
    "            all_targets.append(batch.next_positions.view(-1, 2).cpu().numpy())\n",
    "            all_batch_info.append({\n",
    "                'batch_size': batch.graphs.batch.max().item() + 1,\n",
    "                'nodes_per_graph': torch.bincount(batch.graphs.batch).cpu().numpy()\n",
    "            })\n",
    "    \n",
    "    return all_predictions, all_targets, all_batch_info\n",
    "\n",
    "def calculate_position_error_metrics(predictions: List[np.ndarray], targets: List[np.ndarray]) -> Dict:\n",
    "    \"\"\"Calculate intuitive position error metrics\"\"\"\n",
    "    all_pred = np.vstack(predictions)\n",
    "    all_target = np.vstack(targets)\n",
    "    \n",
    "    # Euclidean distance errors\n",
    "    position_errors = np.linalg.norm(all_pred - all_target, axis=1)\n",
    "    \n",
    "    # Direction errors (angle between predicted and actual movement)\n",
    "    direction_errors = []\n",
    "    for pred, target in zip(predictions, targets):\n",
    "        if len(pred) > 1:\n",
    "            pred_directions = pred[1:] - pred[:-1]\n",
    "            target_directions = target[1:] - target[:-1]\n",
    "            \n",
    "            # Calculate angle between vectors\n",
    "            for pd, td in zip(pred_directions, target_directions):\n",
    "                if np.linalg.norm(pd) > 0 and np.linalg.norm(td) > 0:\n",
    "                    cos_angle = np.dot(pd, td) / (np.linalg.norm(pd) * np.linalg.norm(td))\n",
    "                    cos_angle = np.clip(cos_angle, -1, 1)  # Handle numerical errors\n",
    "                    angle_error = np.arccos(cos_angle) * 180 / np.pi\n",
    "                    direction_errors.append(angle_error)\n",
    "    \n",
    "    return {\n",
    "        'mean_position_error': np.mean(position_errors),\n",
    "        'std_position_error': np.std(position_errors),\n",
    "        'median_position_error': np.median(position_errors),\n",
    "        'max_position_error': np.max(position_errors),\n",
    "        'position_errors': position_errors,\n",
    "        'mean_direction_error': np.mean(direction_errors) if direction_errors else 0,\n",
    "        'direction_errors': direction_errors\n",
    "    }\n",
    "\n",
    "def calculate_success_rates(predictions: List[np.ndarray], targets: List[np.ndarray], \n",
    "                          tolerance_levels: List[float] = [0.5, 1.0, 1.5, 2.0]) -> Dict:\n",
    "    \"\"\"Calculate success rates at different tolerance levels\"\"\"\n",
    "    all_pred = np.vstack(predictions)\n",
    "    all_target = np.vstack(targets)\n",
    "    \n",
    "    position_errors = np.linalg.norm(all_pred - all_target, axis=1)\n",
    "    \n",
    "    success_rates = {}\n",
    "    for tolerance in tolerance_levels:\n",
    "        success_rate = np.mean(position_errors <= tolerance)\n",
    "        success_rates[f'success_rate_{tolerance}'] = success_rate\n",
    "    \n",
    "    return success_rates\n",
    "\n",
    "def multi_step_prediction_accuracy(model, test_loader, num_steps: int = 10, device='cpu'):\n",
    "    \"\"\"Evaluate multi-step prediction accuracy\"\"\"\n",
    "    model.eval()\n",
    "    step_errors = {i: [] for i in range(1, num_steps + 1)}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(test_loader):\n",
    "            if batch_idx >= 10:  # Limit for computation\n",
    "                break\n",
    "                \n",
    "            current_graphs = batch.graphs.to(device)\n",
    "            \n",
    "            # Auto-regressive prediction\n",
    "            predictions = []\n",
    "            for step in range(num_steps):\n",
    "                time_span = torch.tensor([0., 1.], device=device)\n",
    "                result = model(current_graphs, time_span)\n",
    "                pred_positions = result['trajectories'][1]\n",
    "                predictions.append(pred_positions.cpu().numpy())\n",
    "                \n",
    "                # Update graph for next prediction (simplified)\n",
    "                # In practice, you'd need to update the graph structure properly\n",
    "                \n",
    "            # Compare with ground truth (if available)\n",
    "            # This is a simplified version - you'd need actual multi-step ground truth\n",
    "            \n",
    "    return step_errors\n",
    "\n",
    "def analyze_collision_prediction(predictions: List[np.ndarray], targets: List[np.ndarray], \n",
    "                               batch_info: List[Dict], collision_threshold: float = 1.5) -> Dict:\n",
    "    \"\"\"Analyze collision prediction accuracy\"\"\"\n",
    "    pred_collisions = []\n",
    "    actual_collisions = []\n",
    "    \n",
    "    for pred, target, info in zip(predictions, targets, batch_info):\n",
    "        nodes_per_graph = info['nodes_per_graph']\n",
    "        start_idx = 0\n",
    "        \n",
    "        for graph_nodes in nodes_per_graph:\n",
    "            if graph_nodes <= 1:\n",
    "                start_idx += graph_nodes\n",
    "                continue\n",
    "                \n",
    "            # Extract positions for this graph\n",
    "            graph_pred = pred[start_idx:start_idx + graph_nodes]\n",
    "            graph_target = target[start_idx:start_idx + graph_nodes]\n",
    "            \n",
    "            # Count collisions in predictions\n",
    "            pred_distances = np.linalg.norm(graph_pred[:, None] - graph_pred[None, :], axis=2)\n",
    "            pred_collisions.append(np.sum((pred_distances < collision_threshold) & (pred_distances > 0)) // 2)\n",
    "            \n",
    "            # Count collisions in targets\n",
    "            target_distances = np.linalg.norm(graph_target[:, None] - graph_target[None, :], axis=2)\n",
    "            actual_collisions.append(np.sum((target_distances < collision_threshold) & (target_distances > 0)) // 2)\n",
    "            \n",
    "            start_idx += graph_nodes\n",
    "    \n",
    "    return {\n",
    "        'predicted_collisions': pred_collisions,\n",
    "        'actual_collisions': actual_collisions,\n",
    "        'collision_prediction_mse': np.mean((np.array(pred_collisions) - np.array(actual_collisions)) ** 2),\n",
    "        'collision_prediction_mae': np.mean(np.abs(np.array(pred_collisions) - np.array(actual_collisions)))\n",
    "    }\n",
    "\n",
    "def plot_prediction_accuracy_analysis(error_metrics: Dict, success_rates: Dict, \n",
    "                                    collision_analysis: Dict, save_path: Optional[str] = None):\n",
    "    \"\"\"Plot comprehensive prediction accuracy analysis\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 1. Position error distribution\n",
    "    axes[0,0].hist(error_metrics['position_errors'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0,0].axvline(error_metrics['mean_position_error'], color='red', linestyle='--', \n",
    "                     label=f'Mean: {error_metrics[\"mean_position_error\"]:.2f}')\n",
    "    axes[0,0].axvline(error_metrics['median_position_error'], color='orange', linestyle='--',\n",
    "                     label=f'Median: {error_metrics[\"median_position_error\"]:.2f}')\n",
    "    axes[0,0].set_title('Position Error Distribution')\n",
    "    axes[0,0].set_xlabel('Position Error (grid units)')\n",
    "    axes[0,0].set_ylabel('Frequency')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Success rates at different tolerances\n",
    "    tolerances = [float(k.split('_')[-1]) for k in success_rates.keys()]\n",
    "    rates = list(success_rates.values())\n",
    "    \n",
    "    axes[0,1].bar(tolerances, rates, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "    axes[0,1].set_title('Prediction Success Rate by Tolerance')\n",
    "    axes[0,1].set_xlabel('Tolerance (grid units)')\n",
    "    axes[0,1].set_ylabel('Success Rate')\n",
    "    axes[0,1].set_ylim(0, 1)\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add percentage labels on bars\n",
    "    for i, rate in enumerate(rates):\n",
    "        axes[0,1].text(tolerances[i], rate + 0.01, f'{rate:.1%}', ha='center', va='bottom')\n",
    "    \n",
    "    # 3. Direction error distribution (if available)\n",
    "    if error_metrics['direction_errors']:\n",
    "        axes[0,2].hist(error_metrics['direction_errors'], bins=30, alpha=0.7, \n",
    "                      color='lightcoral', edgecolor='black')\n",
    "        axes[0,2].axvline(error_metrics['mean_direction_error'], color='red', linestyle='--',\n",
    "                         label=f'Mean: {error_metrics[\"mean_direction_error\"]:.1f}°')\n",
    "        axes[0,2].set_title('Direction Error Distribution')\n",
    "        axes[0,2].set_xlabel('Direction Error (degrees)')\n",
    "        axes[0,2].set_ylabel('Frequency')\n",
    "        axes[0,2].legend()\n",
    "        axes[0,2].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[0,2].text(0.5, 0.5, 'Direction Error\\nNot Available', ha='center', va='center',\n",
    "                      transform=axes[0,2].transAxes, fontsize=12)\n",
    "        axes[0,2].set_title('Direction Error Distribution')\n",
    "    \n",
    "    # 4. Error statistics summary\n",
    "    error_stats = [\n",
    "        error_metrics['mean_position_error'],\n",
    "        error_metrics['median_position_error'],\n",
    "        error_metrics['std_position_error'],\n",
    "        error_metrics['max_position_error']\n",
    "    ]\n",
    "    stat_labels = ['Mean', 'Median', 'Std Dev', 'Max']\n",
    "    \n",
    "    bars = axes[1,0].bar(stat_labels, error_stats, alpha=0.7, color='gold', edgecolor='black')\n",
    "    axes[1,0].set_title('Position Error Statistics')\n",
    "    axes[1,0].set_ylabel('Error (grid units)')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, stat in zip(bars, error_stats):\n",
    "        axes[1,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                      f'{stat:.2f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 5. Collision prediction accuracy\n",
    "    pred_collisions = collision_analysis['predicted_collisions']\n",
    "    actual_collisions = collision_analysis['actual_collisions']\n",
    "    \n",
    "    axes[1,1].scatter(actual_collisions, pred_collisions, alpha=0.6, color='purple')\n",
    "    max_collisions = max(max(pred_collisions) if pred_collisions else 0, \n",
    "                        max(actual_collisions) if actual_collisions else 0)\n",
    "    axes[1,1].plot([0, max_collisions], [0, max_collisions], 'r--', label='Perfect Prediction')\n",
    "    axes[1,1].set_title('Collision Prediction Accuracy')\n",
    "    axes[1,1].set_xlabel('Actual Collisions')\n",
    "    axes[1,1].set_ylabel('Predicted Collisions')\n",
    "    axes[1,1].legend()\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Performance summary\n",
    "    summary_text = f\"\"\"Model Performance Summary\n",
    "    \n",
    "Position Accuracy:\n",
    "• Mean Error: {error_metrics['mean_position_error']:.2f} units\n",
    "• Success Rate (≤1.0): {success_rates.get('success_rate_1.0', 0):.1%}\n",
    "• Success Rate (≤1.5): {success_rates.get('success_rate_1.5', 0):.1%}\n",
    "\n",
    "Direction Accuracy:\n",
    "• Mean Direction Error: {error_metrics['mean_direction_error']:.1f}°\n",
    "\n",
    "Collision Prediction:\n",
    "• MAE: {collision_analysis['collision_prediction_mae']:.2f}\n",
    "• MSE: {collision_analysis['collision_prediction_mse']:.2f}\n",
    "\"\"\"\n",
    "    \n",
    "    axes[1,2].text(0.05, 0.95, summary_text, transform=axes[1,2].transAxes, \n",
    "                  fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
    "                  bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "    axes[1,2].set_xlim(0, 1)\n",
    "    axes[1,2].set_ylim(0, 1)\n",
    "    axes[1,2].axis('off')\n",
    "    axes[1,2].set_title('Performance Summary')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_model_comprehensive(model, test_loader, device='cpu', max_batches=20, save_dir='./evaluation_results/'):\n",
    "    \"\"\"Comprehensive model evaluation with intuitive metrics\"\"\"\n",
    "    import os\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    print(\"Generating predictions...\")\n",
    "    predictions, targets, batch_info = predict_trajectories(model, test_loader, device, max_batches)\n",
    "    \n",
    "    print(\"Calculating error metrics...\")\n",
    "    error_metrics = calculate_position_error_metrics(predictions, targets)\n",
    "    \n",
    "    print(\"Calculating success rates...\")\n",
    "    success_rates = calculate_success_rates(predictions, targets)\n",
    "    \n",
    "    print(\"Analyzing collision prediction...\")\n",
    "    collision_analysis = analyze_collision_prediction(predictions, targets, batch_info)\n",
    "    \n",
    "    print(\"Creating visualizations...\")\n",
    "    plot_prediction_accuracy_analysis(error_metrics, success_rates, collision_analysis, \n",
    "                                    f\"{save_dir}/prediction_accuracy_analysis.png\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"MODEL EVALUATION SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Mean Position Error: {error_metrics['mean_position_error']:.3f} grid units\")\n",
    "    print(f\"Median Position Error: {error_metrics['median_position_error']:.3f} grid units\")\n",
    "    print(f\"Success Rate (≤1.0 units): {success_rates.get('success_rate_1.0', 0):.1%}\")\n",
    "    print(f\"Success Rate (≤1.5 units): {success_rates.get('success_rate_1.5', 0):.1%}\")\n",
    "    print(f\"Mean Direction Error: {error_metrics['mean_direction_error']:.1f} degrees\")\n",
    "    print(f\"Collision Prediction MAE: {collision_analysis['collision_prediction_mae']:.3f}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    return {\n",
    "        'error_metrics': error_metrics,\n",
    "        'success_rates': success_rates,\n",
    "        'collision_analysis': collision_analysis\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928e887d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphODEFunc(nn.Module):    \n",
    "    def __init__(self, node_dim: int, hidden_dim: int = 64, num_layers: int = 2):\n",
    "        super().__init__()\n",
    "        self.node_dim = node_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # GraphSAGE layers\n",
    "        self.conv1 = SAGEConv(node_dim, hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = SAGEConv(hidden_dim, node_dim)\n",
    "        \n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, t: torch.Tensor, x: torch.Tensor, \n",
    "                edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        h = self.conv1(x, edge_index)\n",
    "        h = self.activation(h)\n",
    "        \n",
    "        h = self.conv2(h, edge_index)\n",
    "        h = self.activation(h)\n",
    "        \n",
    "        # Final layer (no activation for derivative)\n",
    "        dx_dt = self.conv3(h, edge_index)\n",
    "        \n",
    "        return dx_dt\n",
    "\n",
    "class GraphODE(nn.Module):\n",
    "    \"\"\"Simple Graph Neural ODE for trajectory prediction\"\"\"\n",
    "    \n",
    "    def __init__(self, node_dim: int, num_agvs: int, num_pickers: int, hidden_dim: int = 64, ode_solver: str = 'euler'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.node_dim = node_dim\n",
    "        self.num_agvs = num_agvs\n",
    "        self.num_pickers = num_pickers\n",
    "        self.ode_solver = ode_solver\n",
    "        \n",
    "        # ODE function\n",
    "        self.ode_func = GraphODEFunc(\n",
    "            node_dim=node_dim,\n",
    "            hidden_dim=hidden_dim\n",
    "        )\n",
    "        \n",
    "        # Simple position decoder\n",
    "        self.position_decoder = nn.Linear(node_dim, 2)\n",
    "        \n",
    "    def forward(self, batch_data: Batch, time_span: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "\n",
    "        x0 = batch_data.x  # [total_nodes, node_dim]\n",
    "        edge_index = batch_data.edge_index\n",
    "        batch = batch_data.batch  # [total_nodes] batch indices\n",
    "\n",
    "        # Create ODE function with fixed graph structure\n",
    "        def ode_func_wrapper(t, x):\n",
    "            return self.ode_func(t, x, edge_index)\n",
    "        \n",
    "        # Solve ODE\n",
    "        solution = odeint(\n",
    "            ode_func_wrapper,\n",
    "            x0,\n",
    "            time_span,\n",
    "            method=self.ode_solver,\n",
    "            rtol=1e-3,\n",
    "            atol=1e-4\n",
    "        )  # [num_time_points, total_nodes, node_dim]\n",
    "        \n",
    "        # Extract trajectories (positions) at all time points\n",
    "        trajectories = []\n",
    "        for t_idx in range(solution.size(0)):\n",
    "            node_features = solution[t_idx]  # [total_nodes, node_dim]\n",
    "            positions = self.position_decoder(node_features)  # [total_nodes, 2]\n",
    "            trajectories.append(positions)\n",
    "\n",
    "        trajectories = torch.stack(trajectories, dim=0)  # [num_time_points, total_nodes, 2]\n",
    "\n",
    "        return {\n",
    "            'trajectories': trajectories,\n",
    "            'node_features': solution,\n",
    "            'batch': batch\n",
    "        }\n",
    "    \n",
    "    def predict_trajectory(self, batch_data: Batch, num_steps: int, dt: float = 0.1) -> torch.Tensor:\n",
    "\n",
    "        time_span = torch.arange(0, num_steps + 1, dtype=torch.float32)\n",
    "        result = self.forward(batch_data, time_span)\n",
    "        return result['trajectories']\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c479e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryBatch:\n",
    "    def __init__(self, graphs: Batch, next_positions: torch.Tensor):\n",
    "        self.graphs = graphs\n",
    "        self.next_positions = next_positions\n",
    "\n",
    "class WarehouseDataset(Dataset):\n",
    "    def __init__(self, h5_file_path: str):\n",
    "        self.h5_file_path = h5_file_path\n",
    "\n",
    "        with h5py.File(h5_file_path, 'r') as f:\n",
    "            episode_ids = [int(key.split('_')[1]) for key in f.keys() if key.startswith('episode_')]\n",
    "        self.episode_ids = sorted(episode_ids)\n",
    "\n",
    "        self.sequences = []\n",
    "        self.node_dim = None\n",
    "        self.num_agvs = None\n",
    "        self.num_pickers = None\n",
    "\n",
    "        self._load_all_sequences()\n",
    "\n",
    "    def _load_all_sequences(self):\n",
    "        with h5py.File(self.h5_file_path, 'r') as f:\n",
    "            episode_ids = [int(key.split('_')[1]) for key in f.keys() if key.startswith('episode_')]\n",
    "            self.episode_ids = sorted(episode_ids)\n",
    "            \n",
    "            for episode_id in self.episode_ids:\n",
    "                episode_group = f[f'episode_{episode_id:06d}']\n",
    "                steps_group = episode_group['steps']\n",
    "                num_agvs = episode_group['metadata'].attrs['num_agvs']\n",
    "                num_pickers = episode_group['metadata'].attrs['num_pickers']\n",
    "                \n",
    "                if self.num_agvs is None:\n",
    "                    self.num_agvs = num_agvs\n",
    "                    self.num_pickers = num_pickers\n",
    "                    \n",
    "                converter = GraphConverter(num_agvs, num_pickers, distance_threshold=5.0, temporal_window=5)\n",
    "                step_data = []\n",
    "                \n",
    "                for step_name in sorted(steps_group.keys()):\n",
    "                    step_group = steps_group[step_name]\n",
    "                    observations = step_group['observations'][:]\n",
    "                    graph_data = converter._build_graph_from_observation(observations)\n",
    "                    \n",
    "                    if self.node_dim is None:\n",
    "                        self.node_dim = graph_data.x.size(1)\n",
    "                        \n",
    "                    positions = self._extract_positions_from_graph(graph_data, num_agvs, num_pickers)\n",
    "                    step_data.append({\n",
    "                        'graph': graph_data,\n",
    "                        'positions': positions,\n",
    "                    })\n",
    "                \n",
    "                # (현재 그래프, 다음 위치) 페어 생성\n",
    "                for i in range(len(step_data) - 1):  # 마지막 스텝은 다음이 없으니 제외\n",
    "                    current_graph = step_data[i]['graph']\n",
    "                    next_positions = step_data[i + 1]['positions']\n",
    "                    \n",
    "                    self.sequences.append(TrajectoryBatch(current_graph, next_positions))\n",
    "                    \n",
    "        print(f\"Loaded {len(self.sequences)} step pairs from {self.h5_file_path}\")\n",
    "        print(f\"Node dimension: {self.node_dim}\")\n",
    "        print(f\"Agents: {self.num_agvs} AGVs, {self.num_pickers} Pickers\")\n",
    "    \n",
    "    def _extract_positions_from_graph(self, graph: Data, num_agvs: int, num_pickers: int) -> torch.Tensor:\n",
    "        \"\"\"Extract position information from graph\"\"\"\n",
    "        positions = []\n",
    "        \n",
    "        # AGV positions (indices 3, 4 in standardized obs -> y, x -> x, y)\n",
    "        if num_agvs > 0:\n",
    "            agv_features = graph.x[:num_agvs]\n",
    "            agv_pos = agv_features[:, [4, 3]]  # [x, y]\n",
    "            positions.append(agv_pos)\n",
    "        \n",
    "        # Picker positions (indices 0, 1 in standardized obs -> y, x -> x, y)  \n",
    "        if num_pickers > 0:\n",
    "            picker_features = graph.x[num_agvs:num_agvs + num_pickers]\n",
    "            picker_pos = picker_features[:, [1, 0]]  # [x, y]\n",
    "            positions.append(picker_pos)\n",
    "        \n",
    "        # Concatenate all positions\n",
    "        all_positions = torch.cat(positions, dim=0)  # [total_agents, 2]\n",
    "        \n",
    "        return all_positions\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx]\n",
    "\n",
    "def collate_trajectory_batches(batch_list: List[TrajectoryBatch]) -> TrajectoryBatch:\n",
    "    \"\"\"Collate function for DataLoader\"\"\"\n",
    "    # Batch graphs\n",
    "    graphs = [item.graphs for item in batch_list]\n",
    "    batched_graph = Batch.from_data_list(graphs)\n",
    "    \n",
    "    # Stack next positions\n",
    "    next_positions = torch.stack([item.next_positions for item in batch_list], dim=0)  # [B, N, 2]\n",
    "    \n",
    "    return TrajectoryBatch(\n",
    "        graphs=batched_graph,\n",
    "        next_positions=next_positions\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016681b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "environments = [\n",
    "    'tarware-tiny-3agvs-2pickers-partialobs-v1',\n",
    "    'tarware-small-6agvs-3pickers-partialobs-v1', \n",
    "    'tarware-medium-10agvs-5pickers-partialobs-v1',\n",
    "    'tarware-medium-19agvs-9pickers-partialobs-v1',\n",
    "    'tarware-large-15agvs-8pickers-partialobs-v1'\n",
    "]\n",
    "env_name = ['tiny-3-2', 'sml-6-3', 'med-10-5', 'med-19-9', 'lar-15-8']\n",
    "model_paths = [f'./trained_models/{env}' for env in environments]\n",
    "file_dict = {name: path for name, path in zip(env_name, model_paths)}\n",
    "dataset_path = [f'./test_{env}.h5' for env in environments]\n",
    "for name, path in file_dict.items():\n",
    "    print(f\"{name}: {path}\")\n",
    "    test_dataset = WarehouseDataset(dataset_path[env_name.index(name)])\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_trajectory_batches)\n",
    "    node_dim = test_dataset[0].graphs.x.size(1)\n",
    "    num_agvs = env_name.split('-')[1]\n",
    "    num_pickers = env_name.split('-')[2]\n",
    "    model = load_trained_model(f\"{path}/model.pt\", GraphODE, node_dim = node_dim, num_agvs=num_agvs, num_pickers=num_pickers, hidden_dim=64, ode_solver='euler')\n",
    "    results = evaluate_model_comprehensive(model, test_loader, device='cpu', max_batches=20, save_dir=f'./evaluation_results/{name}/')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
